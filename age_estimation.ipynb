{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fausto1364/age-estimation/blob/main/age_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oP948sesdx2"
      },
      "source": [
        "Age estimation using AlexNet based architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iA8uNcAfAV2O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# image manipulation\n",
        "from PIL import Image\n",
        "\n",
        "# deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9263BDgsXv7",
        "outputId": "32bee66e-a070-48d8-ffab-6383637857ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount drive to access APPA-REAL\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path='/content/drive/MyDrive/appa-real-release'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rxaCu1-Dg5l"
      },
      "outputs": [],
      "source": [
        "# just to store comands for Image... Will return error\n",
        "image = Image.open(os.path.join(path,'005612.jpg_face.jpg'))\n",
        "image.show()\n",
        "print(image.format)\n",
        "print(image.mode)\n",
        "print(image.size)\n",
        "test = image.resize((64,64))\n",
        "test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D5GtgdloFV9_"
      },
      "outputs": [],
      "source": [
        "# extracts only face detected jpegs. Then rescales size of image and converts to numpy matrix\n",
        "def preprocess(folder_path,size):\n",
        "  X=[] # contains processed images\n",
        "  y=[] # contains ids of images\n",
        "  # regular expression pattern to only extract files with face detection\n",
        "  pattern = r\"\\d+\\.jpg_face\\.jpg\"\n",
        "\n",
        "  # Loop over all files in the folder\n",
        "  for imagename in os.listdir(folder_path):\n",
        "    image_path = os.path.join(folder_path, imagename)\n",
        "    \n",
        "    if re.match(pattern, imagename):\n",
        "      image = Image.open(image_path)\n",
        "      image = image.resize(size)\n",
        "      X.append(np.array(image.getdata()).reshape(image.size[1], image.size[0], 3))\n",
        "      y.append(imagename.split(\".\")[0])\n",
        "  return X,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-In5DH1SvJzW"
      },
      "outputs": [],
      "source": [
        "# read the csv files with the apparent ages\n",
        "def get_appaages(path):\n",
        "  id = []\n",
        "  appa_age = []\n",
        "\n",
        "  # Read the CSV file\n",
        "  with open(path, \"r\") as csv_file:\n",
        "    csv_reader = csv.reader(csv_file)\n",
        "    \n",
        "    # Skip header\n",
        "    next(csv_reader)\n",
        "    \n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in csv_reader:\n",
        "        # Append the values of the first and third columns to their respective lists\n",
        "        id.append(row[0].split(\".\")[0])\n",
        "        appa_age.append(row[2])\n",
        "\n",
        "  return appa_age, id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2t9O9lW1CFmK"
      },
      "outputs": [],
      "source": [
        "# allign two lists according to lists with ids\n",
        "def allign_lists(A,a,B,b):\n",
        "  X=[]\n",
        "  Y=[]\n",
        "  for i in range(len(A)):\n",
        "    X.append(A[i])\n",
        "    Y.append(np.float32(B[b.index(a[i])]))\n",
        "\n",
        "  return np.array(X),np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G5fPAm-7wM4J"
      },
      "outputs": [],
      "source": [
        "# extract and preprocess images\n",
        "train_path = os.path.join(path, 'train')\n",
        "valid_path = os.path.join(path, 'valid')\n",
        "test_path = os.path.join(path, 'test')\n",
        "\n",
        "images_train,ids_train_im = preprocess(train_path,(64,64))\n",
        "images_valid,ids_valid_im = preprocess(valid_path,(64,64))\n",
        "images_test,ids_test_im = preprocess(test_path,(64,64))\n",
        "\n",
        "# extract apparent ages\n",
        "gt_train = os.path.join(path, 'gt_avg_train.csv')\n",
        "gt_valid = os.path.join(path, 'gt_avg_valid.csv')\n",
        "gt_test = os.path.join(path, 'gt_avg_test.csv')\n",
        "\n",
        "appaages_train, ids_train_gt = get_appaages(gt_train)\n",
        "appaages_valid, ids_valid_gt = get_appaages(gt_valid)\n",
        "appaages_test, ids_test_gt = get_appaages(gt_test)\n",
        "\n",
        "# sort apparent ages by images through the ids\n",
        "images_train, appaages_train = allign_lists(images_train, ids_train_im, appaages_train, ids_train_gt)\n",
        "images_valid, appaages_valid = allign_lists(images_valid, ids_valid_im, appaages_valid, ids_valid_gt)\n",
        "images_test, appaages_test = allign_lists(images_test, ids_test_im, appaages_test, ids_test_gt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HaFZZaPu49GM"
      },
      "outputs": [],
      "source": [
        "# Convert data to PyTorch tensors\n",
        "train_data = data.TensorDataset(torch.tensor(images_train), torch.tensor(appaages_train))\n",
        "test_data = data.TensorDataset(torch.tensor(images_test), torch.tensor(appaages_test))\n",
        "valid_data = data.TensorDataset(torch.tensor(images_valid), torch.tensor(appaages_valid))\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_dataloader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = data.DataLoader(test_data, batch_size=batch_size)\n",
        "valid_dataloader = data.DataLoader(valid_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3QEwE2Zgs9nI"
      },
      "outputs": [],
      "source": [
        "# do the architecture. Basic, simple one\n",
        "class AgeEstimationCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AgeEstimationCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(16 * 32 * 32, 256)\n",
        "        self.fc2 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 32 * 32)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some weird problems with this architecture from (https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/)\n",
        "'''class LeNet(nn.Module):\n",
        "    def __init__(self, numChannels):\n",
        "\t\t    # call the parent constructor\n",
        "\t\t    super(LeNet, self).__init__()\n",
        "\t\t    # initialize first set of CONV => RELU => POOL layers\n",
        "\t\t    self.conv1 = nn.Conv2d(in_channels=numChannels, out_channels=20, kernel_size=(5, 5))\n",
        "\t\t    self.relu1 = nn.ReLU()\n",
        "\t\t    self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\t\t    # initialize second set of CONV => RELU => POOL layers\n",
        "\t\t    self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
        "\t\t    self.relu2 = nn.ReLU()\n",
        "\t\t    self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\t\t    # initialize first (and only) set of FC => RELU layers\n",
        "\t\t    self.fc1 = nn.Linear(in_features=800, out_features=500)\n",
        "\t\t    self.relu3 = nn.ReLU()\n",
        "\t\t    # initialize our softmax classifier\n",
        "\t\t    self.fc2 = nn.Linear(in_features=500, out_features=1)\n",
        "\t\t    self.logSoftmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "    def forward(self, x):\n",
        "\t\t    # pass the input through our first set of CONV => RELU =>\n",
        "\t\t    # POOL layers\n",
        "\t\tx = self.conv1(x)\n",
        "\t\tx = self.relu1(x)\n",
        "\t\tx = self.maxpool1(x)\n",
        "\t\t# pass the output from the previous layer through the second\n",
        "\t\t# set of CONV => RELU => POOL layers\n",
        "\t\tx = self.conv2(x)\n",
        "\t\tx = self.relu2(x)\n",
        "\t\tx = self.maxpool2(x)\n",
        "\t\t# flatten the output from the previous layer and pass it\n",
        "\t\t# through our only set of FC => RELU layers\n",
        "\t\tx = nn.flatten(x, 1)\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.relu3(x)\n",
        "\t\t# pass the output to our softmax classifier to get our output\n",
        "\t\t# predictions\n",
        "\t\tx = self.fc2(x)\n",
        "\t\toutput = self.logSoftmax(x)\n",
        "\t\t# return the output predictions\n",
        "\t\treturn output'''"
      ],
      "metadata": {
        "id": "FL6CjRsjex6G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "j7rUneFq60m3"
      },
      "outputs": [],
      "source": [
        "model = AgeEstimationCNN()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0pfxmQpe64Ii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "cac6d5ec-bda0-479b-f206-b8a00d9fb7c6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-c4274a8611a3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#inputs = inputs.float()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-70b75bc9e311>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (long int) and bias type (float) should be the same"
          ]
        }
      ],
      "source": [
        "# do the training\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        #inputs = inputs.float()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print average loss for each epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/len(train_dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLMDVKeQtDiT"
      },
      "outputs": [],
      "source": [
        "# do the evaluation\n",
        "model.eval()\n",
        "total_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(test_dataloader)\n",
        "    print(f\"Test Loss: {average_loss:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11r4otEFT2crPr0bsoxXYV_XxY-Yr0VHA",
      "authorship_tag": "ABX9TyPVthrgUODYVh6crhHNPEj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}